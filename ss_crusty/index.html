<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="generator" content="rustdoc"><meta name="description" content="ss_crusty (Simple Scraper) is a high-performance, command-line application written in Rust ğŸ¦€ designed to fetch HTML content from a list of URLs in parallel and extract all absolute links. GitHub Repo"><title>ss_crusty - Rust</title><script>if(window.location.protocol!=="file:")document.head.insertAdjacentHTML("beforeend","SourceSerif4-Regular-6b053e98.ttf.woff2,FiraSans-Italic-81dc35de.woff2,FiraSans-Regular-0fe48ade.woff2,FiraSans-MediumItalic-ccf7e434.woff2,FiraSans-Medium-e1aa3f0a.woff2,SourceCodePro-Regular-8badfe75.ttf.woff2,SourceCodePro-Semibold-aa29a496.ttf.woff2".split(",").map(f=>`<link rel="preload" as="font" type="font/woff2"href="../static.files/${f}">`).join(""))</script><link rel="stylesheet" href="../static.files/normalize-9960930a.css"><link rel="stylesheet" href="../static.files/rustdoc-e56847b5.css"><meta name="rustdoc-vars" data-root-path="../" data-static-root-path="../static.files/" data-current-crate="ss_crusty" data-themes="" data-resource-suffix="" data-rustdoc-version="1.91.0 (f8297e351 2025-10-28)" data-channel="1.91.0" data-search-js="search-e256b49e.js" data-stringdex-js="stringdex-c3e638e9.js" data-settings-js="settings-c38705f0.js" ><script src="../static.files/storage-e2aeef58.js"></script><script defer src="../crates.js"></script><script defer src="../static.files/main-6dc2a7f3.js"></script><noscript><link rel="stylesheet" href="../static.files/noscript-263c88ec.css"></noscript><link rel="alternate icon" type="image/png" href="../static.files/favicon-32x32-eab170b8.png"><link rel="icon" type="image/svg+xml" href="../static.files/favicon-044be391.svg"></head><body class="rustdoc mod crate"><!--[if lte IE 11]><div class="warning">This old browser is unsupported and will most likely display funky things.</div><![endif]--><rustdoc-topbar><h2><a href="#">Crate ss_crusty</a></h2></rustdoc-topbar><nav class="sidebar"><div class="sidebar-crate"><h2><a href="../ss_crusty/index.html">ss_<wbr>crusty</a><span class="version">0.2.1</span></h2></div><div class="sidebar-elems"><ul class="block"><li><a id="all-types" href="all.html">All Items</a></li></ul><section id="rustdoc-toc"><h3><a href="#">Sections</a></h3><ul class="block top-toc"><li><a href="#core-functionality" title="Core Functionality">Core Functionality</a><ul><li><a href="#arguments" title="Arguments">Arguments</a></li><li><a href="#usage" title="Usage">Usage</a></li></ul></li></ul><h3><a href="#modules">Crate Items</a></h3><ul class="block"><li><a href="#modules" title="Modules">Modules</a></li><li><a href="#structs" title="Structs">Structs</a></li><li><a href="#functions" title="Functions">Functions</a></li></ul></section><div id="rustdoc-modnav"></div></div></nav><div class="sidebar-resizer" title="Drag to resize sidebar"></div><main><div class="width-limiter"><section id="main-content" class="content"><div class="main-heading"><h1>Crate <span>ss_<wbr>crusty</span>&nbsp;<button id="copy-path" title="Copy item path to clipboard">Copy item path</button></h1><rustdoc-toolbar></rustdoc-toolbar><span class="sub-heading"><a class="src" href="../src/ss_crusty/main.rs.html#1-106">Source</a> </span></div><details class="toggle top-doc" open><summary class="hideme"><span>Expand description</span></summary><div class="docblock"><p>ss_crusty (Simple Scraper) is a high-performance, command-line application written in <strong>Rust</strong> ğŸ¦€ designed to <strong>fetch HTML content from a list of URLs in parallel and extract all absolute links</strong>.
<a href="https://github.com/Fairdose/ss_crusty">GitHub Repo</a></p>
<h4 id="core-functionality"><a class="doc-anchor" href="#core-functionality">Â§</a>Core Functionality</h4>
<ol>
<li><strong>Input Versatility:</strong> Accepts target URLs both directly via command-line arguments and from external files.</li>
<li><strong>Concurrency (Multi-threading):</strong> Utilizes the <strong><a href="https://docs.rs/rayon/latest/rayon/"><code>rayon</code></a></strong> library for <strong>parallel execution</strong> (<code>par_iter().map(...)</code>), enabling the application to fetch and scrape multiple URLs concurrently across available CPU cores, ensuring fast and efficient processing of large lists.</li>
<li><strong>Data Extraction:</strong> Employs the <a href="https://docs.rs/reqwest/latest/reqwest/"><code>reqwest</code></a> HTTP client and the <a href="https://docs.rs/scraper/latest/scraper/"><code>scraper</code></a> parsing library to robustly fetch and analyze web pages.</li>
<li><strong>Structured Output:</strong> Gathers all results (original URL, raw HTML, and unique extracted links) and serializes them into a single, clean <strong>JSON file</strong> using <a href="https://docs.rs/serde/latest/serde/"><code>serde</code></a>.</li>
</ol>
<hr />
<h5 id="arguments"><a class="doc-anchor" href="#arguments">Â§</a>Arguments</h5>
<ul>
<li><strong><code>--urls &lt;URL&gt;</code></strong>: URLs to fetch. <strong>Must be repeated</strong> for each URL to be added (e.g., <code>--urls "url1" --urls "url2"</code>).</li>
<li><strong><code>--file &lt;PATH&gt;</code></strong>: Path to one or more files containing URLs. <strong>Must be repeated</strong> for each file (e.g., <code>--file "list1.txt" --file "list2.txt"</code>).</li>
<li><strong><code>--output &lt;PATH&gt;</code></strong>: The output JSON file name (defaults to <code>results.json</code>).</li>
<li><strong><code>--user-agent &lt;AGENT:String&gt;</code></strong>: Optional user-agent string override (<code>Mozilla</code>, <code>Webkit</code>, or <code>Chrome</code>).</li>
<li><strong><code>-v, -vv, -vvv</code></strong>: Controls logging level (<code>-v</code> = Info, <code>-vv</code> = Debug, <code>-vvv</code> = Warn).</li>
</ul>
<hr />
<h5 id="usage"><a class="doc-anchor" href="#usage">Â§</a>Usage</h5>
<p>Run the application with URLs and specify the output file:</p>
<div class="example-wrap"><pre class="language-bash"><code>ss_crusty --urls &quot;https://example.com&quot; --file &quot;list.txt&quot; --output &quot;results.json&quot; -vv</code></pre></div></div></details><h2 id="modules" class="section-header">Modules<a href="#modules" class="anchor">Â§</a></h2><dl class="item-table"><dt><a class="mod" href="cli/index.html" title="mod ss_crusty::cli">cli</a><span title="Restricted Visibility">&nbsp;ğŸ”’</span> </dt><dd><strong>cli.rs</strong> handles all user interaction via the command line and configures the HTTP environment âš™ï¸.</dd><dt><a class="mod" href="io/index.html" title="mod ss_crusty::io">io</a><span title="Restricted Visibility">&nbsp;ğŸ”’</span> </dt><dd><strong>io.rs</strong> provides basic utilities for file system interaction ğŸ’¾.</dd><dt><a class="mod" href="scrape/index.html" title="mod ss_crusty::scrape">scrape</a><span title="Restricted Visibility">&nbsp;ğŸ”’</span> </dt><dd><strong>scrape.rs</strong> contains the core logic for network communication and data extraction ğŸ•¸ï¸.</dd></dl><h2 id="structs" class="section-header">Structs<a href="#structs" class="anchor">Â§</a></h2><dl class="item-table"><dt><a class="struct" href="struct.PageResult.html" title="struct ss_crusty::PageResult">Page<wbr>Result</a><span title="Restricted Visibility">&nbsp;ğŸ”’</span> </dt><dd>Represents the result of scraping a single URL.
This structure is used by the application to serialize the final data into the output JSON file.</dd></dl><h2 id="functions" class="section-header">Functions<a href="#functions" class="anchor">Â§</a></h2><dl class="item-table"><dt><a class="fn" href="fn.main.html" title="fn ss_crusty::main">main</a><span title="Restricted Visibility">&nbsp;ğŸ”’</span> </dt></dl></section></div></main></body></html>